# Decision-tree-VS-Random-forest
machine learning model using kaggle
1)What is Decision tree and Random Forest?

Decision Tree 


Decision tree is a Supervised Machine Learning Algorithm that may be used to tackle both regression and classification issues. It constructs the model as a tree structure with decision nodes and leaf nodes. A decision node has two or more branches. A decision is represented by a leaf node. The root node is the highest decision node. Decision Trees handle both category and continuous data.




Random Forest



Random Forest is a tree-based machine learning algorithm that leverages the power of multiple decision trees for making decisions. As the name suggests, it is a “forest” of trees!
But why do we call it a “random” forest? That’s because it is a forest of randomly created decision trees. Each node in the decision tree works on a random subset of features to calculate the output. The random forest then combines the output of individual decision trees to generate the final output. Bootstrapping is the process of randomly selecting items from the training dataset. This is a haphazard technique. It assembles randomized decisions based on several decisions and makes the final decision based on the majority voting.
